{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8267b3e5-ee0a-433a-a6c1-e717135e5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_from_disk\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from dotenv import load_dotenv\n",
    "from utils import log_metrics_and_confusion_matrices_wandb, plot_count_and_normalized_confusion_matrix, process_output\n",
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4239bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "WANDB_PROJECT_NAME = 'llm-classification'\n",
    "experiment_name = 'llama_fine_tuning'\n",
    "model_name = huggingface_model_name = \"dariast/Meta-Llama-3.1-8B-Instruct\"\n",
    "dataset_name = 'euvsdisinfo'\n",
    "api_key = os.getenv('API_KEY')\n",
    "base_url = os.getenv('BASE_URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c5f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "notebook_login() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d92d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login() \n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=WANDB_PROJECT_NAME,\n",
    "    name=experiment_name,\n",
    "    # track hyperparameters and run metadata\n",
    "    config = {\n",
    "        \"model\": model_name,\n",
    "        \"dataset\": dataset_name\n",
    "    }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40fb53f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89f43c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_from_disk(\"/home/dariast/Disinfo_Russian_Media/data/fine_tuning_dataset\")\n",
    "# Access train and test splits\n",
    "train_ds = dataset['train']\n",
    "test_ds = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739c69ce-d208-499c-87f1-537ba17e7e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427d92ed-37c8-46f6-ac93-792bcf96ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ccb9249-3bf0-4de1-9bda-2aa09536fcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.9.post4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",    \n",
    "\n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  \n",
    "    loftq_config = None \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaa03ef-ef7b-4ebd-bdf8-cde93ebdbd1b",
   "metadata": {},
   "source": [
    "lr1 = 2e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae2efe1b-3be2-4c2e-9122-17fc8e8e5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_ds,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 4, # Set this for 1 full training run.\n",
    "        # max_steps = 40,\n",
    "        learning_rate = 3e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fa4dee4-3524-4eba-934b-a584719ccd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA H100 PCIe. Max memory = 79.109 GB.\n",
      "5.906 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a956c",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8bbc3-85ae-47f7-9319-90d87ff6a5bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "416d60e8-649a-4da3-9ad0-1de8de1f5070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "840.0644 seconds used for training.\n",
      "14.0 minutes used for training.\n",
      "Peak reserved memory = 9.158 GB.\n",
      "Peak reserved memory for training = 3.252 GB.\n",
      "Peak reserved memory % of max memory = 11.576 %.\n",
      "Peak reserved memory for training % of max memory = 4.111 %.\n"
     ]
    }
   ],
   "source": [
    "# Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd0098c",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0787c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF save\n",
    "model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "model.push_to_hub_merged(huggingface_model_name, tokenizer, save_method = \"merged_16bit\", token = HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6e78607-392a-42e5-8f22-0dff81b2b5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1d2c64b06044e4bef463b3c9755795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/606 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba6231aeb394a17819f6daef1ce00b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/83.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/dariast/Meta-Llama-3.1-8B-4bit-python\n"
     ]
    }
   ],
   "source": [
    "# Local save\n",
    "model.save_pretrained(\"llama_fine_tuned\") # Local saving\n",
    "tokenizer.save_pretrained(\"llama_fine_tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283af006",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe29881",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub_gguf(\n",
    "    \"dariast/Meta-Llama-3.1-8B-Instruct\", # Change hf to your username!\n",
    "    tokenizer,\n",
    "    quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "    token = HF_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5174ecb",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a3bf5b-119e-450d-abeb-4f70e8cd390d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"dataset/llama_fine_tuned\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype, \n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "040bdd0b-e3f5-4ecd-87dd-e1f638e8da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds['text']\n",
    "def substitute_last_part_of_prompt(input_string):\n",
    "    result = re.sub(r'\\n{3}\"Class\": .*?<\\|eot_id\\|>', '', input_string)\n",
    "    return result\n",
    "test_prompts = [substitute_last_part_of_prompt(prompt) for prompt in test_ds['text']]\n",
    "log_dataframe = pd.DataFrame(test_ds)\n",
    "model_name = 'llama_fine_tuned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48596866-67b6-439c-88c4-8267c8269a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "# Iterate through all test instances\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    generated_output = model.generate(**inputs, streamer=text_streamer, max_new_tokens=10)\n",
    "    predicted_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    predictions.append(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed41f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_outputs = []\n",
    "for output in predictions:\n",
    "    print(f\"Initial output: {output}\")  \n",
    "    processed_outputs.append(process_output(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1097f347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the outputs to the dataframe\n",
    "log_dataframe[f\"predicted_{model_name}\"] = processed_outputs\n",
    "\n",
    "# Log the dataset output to wandb\n",
    "predictions_artifact = wandb.Artifact('predictions', type='outputs')\n",
    "with predictions_artifact.new_file(f'predictions_{model_name}.csv', mode='w', encoding='utf-8') as f:\n",
    "    log_dataframe.to_csv(f)\n",
    "wandb.run.log_artifact(predictions_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c41d0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean true labels \n",
    "labels = log_dataframe['OUTPUT']\n",
    "processed_true_labels = [process_output(label) for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20310ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'llama_fine_tuned'\n",
    "# Measure performance \n",
    "try:    \n",
    "    y_true = processed_true_labels\n",
    "    y_pred = processed_outputs\n",
    "\n",
    "    cm_plot_path, classification_report, metrics = plot_count_and_normalized_confusion_matrix(y_true=y_true, y_pred=y_pred, save_path=\"./img\", figure_title=f'{task_name}_{model_name}')\n",
    "    \n",
    "    log_metrics_and_confusion_matrices_wandb(cm_plot_path=cm_plot_path, classification_report=classification_report, metrics=metrics, task_name=f'{task_name}')\n",
    "\n",
    "except Exception as e:\n",
    "    print('Error computing metrics: ', e)\n",
    "\n",
    "# Finish logging\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
